"""
Response handler for managing AI responses.
"""

import streamlit as st
from typing import Union
from PIL import Image
from ..models.constants import MessageRole, AppConfig
from ..services.gemini_service import GeminiChatService
from ..services.chat_history import ChatHistoryManager
from ..ui.chat import ChatUI
from logger_config import get_logger

logger = get_logger(__name__)


class ResponseHandler:
    """Handles generating and displaying AI responses."""
    
    def __init__(self, chat_service: GeminiChatService):
        """
        Initialize the response handler.
        
        Args:
            chat_service: The Gemini chat service instance
        """
        self.chat_service = chat_service
    
    def handle_response(self, prompt: str, image: Union[Image.Image, None] = None, uploaded_file_ref = None) -> None:
        """
        Generate and display the AI response.
        
        Args:
            prompt: The user's input prompt
            image: Optional PIL Image to include with the prompt
            uploaded_file_ref: Optional file reference from genai.Client().files.upload()
        """
        image_info = " with image" if image else ""
        file_info = " with uploaded file" if uploaded_file_ref else ""
        logger.info(f"Handling user prompt{image_info}{file_info}")
        
        with st.chat_message(MessageRole.ASSISTANT.value):
            with st.status(AppConfig.THINKING_MESSAGE, expanded=False) as status:
                try:
                    # Update status
                    status.update(label=AppConfig.GENERATING_MESSAGE, state="running")
                    
                    # Get streaming response
                    response_stream = self.chat_service.get_response_stream(
                        prompt, 
                        ChatHistoryManager.get_messages(),
                        image=image,
                        uploaded_file_ref=uploaded_file_ref
                    )
                    
                    # Display streamed response
                    full_response = ChatUI.display_streaming_response(response_stream)
                    
                    # Update status
                    status.update(label=AppConfig.COMPLETE_MESSAGE, state="complete")
                    logger.info("Response displayed successfully")
                    
                except Exception as e:
                    # Handle errors gracefully
                    logger.error(f"Error handling response: {str(e)}", exc_info=True)
                    full_response = AppConfig.API_ERROR_TEMPLATE.format(error=str(e))
                    ChatUI.display_error(full_response)
                    status.update(label=AppConfig.ERROR_MESSAGE, state="error")
            
            # Add response to history with timestamp
            ChatHistoryManager.add_message(MessageRole.ASSISTANT.value, full_response)
            
            # Display timestamp for the assistant's response if enabled
            show_timestamps = st.session_state.get("show_timestamps", True)
            if show_timestamps:
                messages = ChatHistoryManager.get_messages()
                if messages:
                    last_message = messages[-1]
                    if "timestamp" in last_message:
                        st.caption(f"üïê {last_message['timestamp']}")
    
    def handle_response_with_plots(self, prompt: str, image: Union[Image.Image, None] = None, uploaded_file_ref = None) -> None:
        """
        Generate and display AI response with plot support.
        
        This method uses non-streaming response to properly extract plots
        generated by Gemini's code execution feature.
        
        Args:
            prompt: The user's input prompt
            image: Optional PIL Image to include with the prompt
            uploaded_file_ref: Optional file reference from genai.Client().files.upload()
        """
        image_info = " with image" if image else ""
        file_info = " with uploaded file" if uploaded_file_ref else ""
        logger.info(f"Handling user prompt with plot support{image_info}{file_info}")
        
        with st.chat_message(MessageRole.ASSISTANT.value):
            with st.status(AppConfig.THINKING_MESSAGE, expanded=False) as status:
                try:
                    # Update status
                    status.update(label=AppConfig.GENERATING_MESSAGE, state="running")
                    
                    # Get response with plots
                    result = self.chat_service.get_response_with_plots(
                        prompt, 
                        ChatHistoryManager.get_messages(),
                        image=image,
                        uploaded_file_ref=uploaded_file_ref
                    )
                    
                    full_response = result['text']
                    plots = result['plots']
                    
                    # Display text response
                    if full_response:
                        st.markdown(full_response)
                    
                    # Display plots if any were generated
                    if plots:
                        logger.info(f"Displaying {len(plots)} plot(s)")
                        for i, plot in enumerate(plots):
                            st.image(plot.image_data, width='stretch')
                            if plot.description:
                                st.caption(plot.description)
                    
                    # Update status
                    status.update(label=AppConfig.COMPLETE_MESSAGE, state="complete")
                    logger.info(f"Response displayed successfully with {len(plots)} plot(s)")
                    
                except Exception as e:
                    # Handle errors gracefully
                    logger.error(f"Error handling response with plots: {str(e)}", exc_info=True)
                    full_response = AppConfig.API_ERROR_TEMPLATE.format(error=str(e))
                    plots = []
                    ChatUI.display_error(full_response)
                    status.update(label=AppConfig.ERROR_MESSAGE, state="error")
            
            # Add response to history with plots
            plot_bytes = [plot.image_data for plot in plots] if plots else None
            ChatHistoryManager.add_message(MessageRole.ASSISTANT.value, full_response, plots=plot_bytes)
            
            # Display timestamp for the assistant's response if enabled
            show_timestamps = st.session_state.get("show_timestamps", True)
            if show_timestamps:
                messages = ChatHistoryManager.get_messages()
                if messages:
                    last_message = messages[-1]
                    if "timestamp" in last_message:
                        st.caption(f"üïê {last_message['timestamp']}")
